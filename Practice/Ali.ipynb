{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import (\n",
    "    ENGLISH_STOP_WORDS,\n",
    "    TfidfVectorizer,\n",
    "    CountVectorizer,\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../Data/arxiv_train.csv\")\n",
    "test_df = pd.read_csv(\"../Data/arxiv_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles: 80000\n",
      "\n",
      "Research Fields\n",
      "1. astro-ph\n",
      "2. cond-mat\n",
      "3. cs\n",
      "4. eess\n",
      "5. hep-ph\n",
      "6. hep-th\n",
      "7. math\n",
      "8. physics\n",
      "9. quant-ph\n",
      "10. stat\n"
     ]
    }
   ],
   "source": [
    "research_fields = sorted(train_df[\"label\"].unique())\n",
    "\n",
    "# Getting number of rows\n",
    "num_rows = train_df.shape[0]\n",
    "print(f\"Number of articles: {num_rows}\\n\")\n",
    "\n",
    "# Printing each unique element in an ordered list format\n",
    "print(\"Research Fields\")\n",
    "for i, element in enumerate(research_fields, start=1):\n",
    "    print(f\"{i}. {element}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub(r\"\\W\", \" \", text)\n",
    "\n",
    "    # Remove single characters\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", text)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r\"\\^[a-zA-Z]\\s+\", \" \", text)\n",
    "\n",
    "    # Substitute multiple spaces with single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text, flags=re.I)\n",
    "\n",
    "    # Remove prefixed 'b'\n",
    "    text = re.sub(r\"^b\\s+\", \"\", text)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Removing stopwords\n",
    "    stopwords = set(ENGLISH_STOP_WORDS)\n",
    "    text = \" \".join([word for word in text.split() if word not in stopwords])\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Cleaning the abstracts\n",
    "train_df[\"cleaned_abstract\"] = train_df[\"abstract\"].apply(clean_text)\n",
    "test_df[\"cleaned_abstract\"] = test_df[\"abstract\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>abstract</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31716</td>\n",
       "      <td>Automatic meeting analysis is an essential f...</td>\n",
       "      <td>eess</td>\n",
       "      <td>automatic meeting analysis essential fundament...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89533</td>\n",
       "      <td>We propose a protocol to encode classical bi...</td>\n",
       "      <td>quant-ph</td>\n",
       "      <td>propose protocol encode classical bits measure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82700</td>\n",
       "      <td>A number of physically intuitive results for...</td>\n",
       "      <td>quant-ph</td>\n",
       "      <td>number physically intuitive results calculatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78830</td>\n",
       "      <td>In the last decade rare-earth hexaborides ha...</td>\n",
       "      <td>physics</td>\n",
       "      <td>decade rare earth hexaborides investigated fun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94948</td>\n",
       "      <td>We introduce the weak barycenter of a family...</td>\n",
       "      <td>stat</td>\n",
       "      <td>introduce weak barycenter family probability d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>74849</td>\n",
       "      <td>Direct Statistical Simulation (DSS) solves t...</td>\n",
       "      <td>physics</td>\n",
       "      <td>direct statistical simulation dss solves equat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>66424</td>\n",
       "      <td>We introduce a notion of a girth-regular gra...</td>\n",
       "      <td>math</td>\n",
       "      <td>introduce notion girth regular graph k regular...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6562</td>\n",
       "      <td>Planet host stars with well-constrained ages...</td>\n",
       "      <td>astro-ph</td>\n",
       "      <td>planet host stars constrained ages provide rar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>84292</td>\n",
       "      <td>Unprecedented increase of complexity and sca...</td>\n",
       "      <td>quant-ph</td>\n",
       "      <td>unprecedented increase complexity scale data e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18822</td>\n",
       "      <td>The usual concepts of topological physics, s...</td>\n",
       "      <td>cond-mat</td>\n",
       "      <td>usual concepts topological physics berry curva...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           abstract     label  \\\n",
       "0       31716    Automatic meeting analysis is an essential f...      eess   \n",
       "1       89533    We propose a protocol to encode classical bi...  quant-ph   \n",
       "2       82700    A number of physically intuitive results for...  quant-ph   \n",
       "3       78830    In the last decade rare-earth hexaborides ha...   physics   \n",
       "4       94948    We introduce the weak barycenter of a family...      stat   \n",
       "5       74849    Direct Statistical Simulation (DSS) solves t...   physics   \n",
       "6       66424    We introduce a notion of a girth-regular gra...      math   \n",
       "7        6562    Planet host stars with well-constrained ages...  astro-ph   \n",
       "8       84292    Unprecedented increase of complexity and sca...  quant-ph   \n",
       "9       18822    The usual concepts of topological physics, s...  cond-mat   \n",
       "\n",
       "                                    cleaned_abstract  \n",
       "0  automatic meeting analysis essential fundament...  \n",
       "1  propose protocol encode classical bits measure...  \n",
       "2  number physically intuitive results calculatio...  \n",
       "3  decade rare earth hexaborides investigated fun...  \n",
       "4  introduce weak barycenter family probability d...  \n",
       "5  direct statistical simulation dss solves equat...  \n",
       "6  introduce notion girth regular graph k regular...  \n",
       "7  planet host stars constrained ages provide rar...  \n",
       "8  unprecedented increase complexity scale data e...  \n",
       "9  usual concepts topological physics berry curva...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "#### Extract features using both TF-IDF and CountVectorizer. Given that we have separate training and testing datasets, we'll apply fit_transform on the training data and transform on the testing data to ensure the model is tested on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df[\"cleaned_abstract\"])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df[\"cleaned_abstract\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(max_features=5000)\n",
    "X_train_count = count_vectorizer.fit_transform(train_df[\"cleaned_abstract\"])\n",
    "X_test_count = count_vectorizer.transform(test_df[\"cleaned_abstract\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Data Preparation\n",
    "#### We convert feature vectors and labels into PyTorch tensors. For illustration, we convert the TF-IDF vectors and corresponding labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(pd.concat([train_df[\"label\"], test_df[\"label\"]]))\n",
    "\n",
    "y_train_encoded = label_encoder.transform(train_df[\"label\"])\n",
    "y_test_encoded = label_encoder.transform(test_df[\"label\"])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor_tfidf = torch.FloatTensor(X_train_tfidf.toarray())\n",
    "y_train_tensor = torch.LongTensor(y_train_encoded)\n",
    "X_test_tensor_tfidf = torch.FloatTensor(X_test_tfidf.toarray())\n",
    "y_test_tensor = torch.LongTensor(y_test_encoded)\n",
    "\n",
    "# Create TensorDatasets and DataLoaders for TF-IDF\n",
    "train_data_tfidf = TensorDataset(X_train_tensor_tfidf, y_train_tensor)\n",
    "test_data_tfidf = TensorDataset(X_test_tensor_tfidf, y_test_tensor)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader_tfidf = DataLoader(train_data_tfidf, shuffle=True, batch_size=batch_size)\n",
    "test_loader_tfidf = DataLoader(test_data_tfidf, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Defining the Neural Network Models\n",
    " ### A. Feedforward Neural Network (FFNN)\n",
    "#### We start by defining a simple FFNN architecture in PyTorch. This model will take the TF-IDF (or CountVectorizer) features as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)  # First hidden layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, output_dim)  # Output layer\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "\n",
    "input_dim = X_train_tensor_tfidf.shape[1]  # This will depend on our feature extraction\n",
    "output_dim = len(label_encoder.classes_)  # Number of unique labels\n",
    "model_ffnn = FFNN(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Recurrent Neural Network (RNN)\n",
    "#### For an RNN model, considering we're working with bag-of-words features, it's a bit unconventional, as RNNs are typically used with sequential data (like the original text). However, for educational purposes or if you're working with sequential data later, here's a basic outline for an RNN model in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(BasicRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assuming x has shape [batch_size, seq_len, input_size]\n",
    "        out, _ = self.rnn(x)\n",
    "        # If out has shape [batch_size, seq_len, hidden_size] and seq_len is treated as 1\n",
    "        # We need to ensure the tensor is correctly shaped for the linear layer\n",
    "        # If the sequence length is 1, out will effectively be 2D after the rnn layer\n",
    "        if out.dim() == 3:  # [batch_size, seq_len, hidden_size]\n",
    "            out = out[:, -1, :]  # Get the outputs of the last time step\n",
    "        elif out.dim() == 2:  # [batch_size, hidden_size]\n",
    "            # No need to index, as there's no sequence length dimension\n",
    "            pass  # out is already correctly shaped\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected output shape from RNN layer\")\n",
    "\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Note: For RNN, input_dim would typically be the size of the embedding dimension\n",
    "hidden_dim = 128  # Number of features in the hidden state\n",
    "num_layers = 1  # Number of stacked RNN layers\n",
    "model_rnn = BasicRNN(input_dim, hidden_dim, output_dim, num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Our Models\n",
    "#### Training a model in PyTorch involves setting up a loss function, an optimizer, and then looping over out training data to make predictions, compute the loss, and update our model parameters.\n",
    "\n",
    "### Training FFNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.534249767780304\n",
      "Epoch 2/10, Loss: 1.531530926990509\n",
      "Epoch 3/10, Loss: 1.530596013736725\n",
      "Epoch 4/10, Loss: 1.5300429203987123\n",
      "Epoch 5/10, Loss: 1.5296407349586487\n",
      "Epoch 6/10, Loss: 1.5290078923225403\n",
      "Epoch 7/10, Loss: 1.5283774554252625\n",
      "Epoch 8/10, Loss: 1.5278866076469422\n",
      "Epoch 9/10, Loss: 1.5273891352653504\n",
      "Epoch 10/10, Loss: 1.5271688702583313\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10  # Number of times to iterate over the entire dataset\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_ffnn.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader_tfidf):\n",
    "        optimizer.zero_grad()\n",
    "        output = model_ffnn(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    # Print average loss for the epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader_tfidf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training RNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Average Loss: 0.6999785131573677\n",
      "Epoch 2/10, Average Loss: 0.4093828803062439\n",
      "Epoch 3/10, Average Loss: 0.3583139921784401\n",
      "Epoch 4/10, Average Loss: 0.32706051809191705\n",
      "Epoch 5/10, Average Loss: 0.30389044005274773\n",
      "Epoch 6/10, Average Loss: 0.28642616223692896\n",
      "Epoch 7/10, Average Loss: 0.2716622624397278\n",
      "Epoch 8/10, Average Loss: 0.25951268746852874\n",
      "Epoch 9/10, Average Loss: 0.2491700305700302\n",
      "Epoch 10/10, Average Loss: 0.23984440425634385\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10  # Define the number of epochs\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_rnn.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0  # Initialize total loss for the epoch\n",
    "    for batch_idx, (data, targets) in enumerate(\n",
    "        train_loader_tfidf\n",
    "    ):  # Assuming train_loader has sequence data\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Reshape data to (batch_size, seq_length, input_size) if not already\n",
    "        # Adjust -1 to actual sequence length if needed.\n",
    "        data = data.view(batch_size, -1, input_dim)\n",
    "\n",
    "        output = model_rnn(data)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()  # Add up batch loss\n",
    "\n",
    "    avg_loss = total_loss / len(\n",
    "        train_loader_tfidf\n",
    "    )  # Calculate average loss for the epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate FFNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNN Model\n",
      "Accuracy: 81.74%\n",
      "F1 Score: 81.58%\n",
      "Precision: 81.62%\n",
      "Recall: 81.71%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Disable gradient computation\n",
    "with torch.no_grad():\n",
    "    # Make predictions\n",
    "    y_pred_ffnn = model_ffnn(X_test_tensor_tfidf).argmax(dim=1).numpy()\n",
    "    y_true_ffnn = y_test_tensor.numpy()\n",
    "\n",
    "# Compute metrics\n",
    "accuracy_ffnn = accuracy_score(y_true_ffnn, y_pred_ffnn)\n",
    "f1_ffnn = f1_score(y_true_ffnn, y_pred_ffnn, average=\"macro\")\n",
    "precision_ffnn = precision_score(y_true_ffnn, y_pred_ffnn, average=\"macro\")\n",
    "recall_ffnn = recall_score(y_true_ffnn, y_pred_ffnn, average=\"macro\")\n",
    "\n",
    "print(\n",
    "    f\"FFNN Model\\nAccuracy: {accuracy_ffnn * 100:.2f}%\\nF1 Score: {f1_ffnn * 100:.2f}%\\nPrecision: {precision_ffnn * 100:.2f}%\\nRecall: {recall_ffnn * 100:.2f}%\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Model\n",
      "Accuracy: 78.21%\n",
      "F1 Score: 78.32%\n",
      "Precision: 78.66%\n",
      "Recall: 78.18%\n"
     ]
    }
   ],
   "source": [
    "# Assuming the RNN expects input of shape [batch_size, seq_len, input_size]\n",
    "# and your BoW features were treated as a sequence of length equal to the number of features\n",
    "with torch.no_grad():\n",
    "    predictions, true_labels = [], []\n",
    "    for (\n",
    "        X_batch,\n",
    "        y_batch,\n",
    "    ) in test_loader_tfidf:  # Ensure this DataLoader is prepared correctly\n",
    "        # No need for reshaping if your DataLoader provides the correct shape\n",
    "        y_pred_rnn = model_rnn(X_batch).argmax(dim=1)\n",
    "        predictions.extend(y_pred_rnn.tolist())\n",
    "        true_labels.extend(y_batch.tolist())\n",
    "\n",
    "# Compute metrics for RNN\n",
    "accuracy_rnn = accuracy_score(true_labels, predictions)\n",
    "f1_rnn = f1_score(true_labels, predictions, average=\"macro\")\n",
    "precision_rnn = precision_score(true_labels, predictions, average=\"macro\")\n",
    "recall_rnn = recall_score(true_labels, predictions, average=\"macro\")\n",
    "\n",
    "print(\n",
    "    f\"RNN Model\\nAccuracy: {accuracy_rnn * 100:.2f}%\\nF1 Score: {f1_rnn * 100:.2f}%\\nPrecision: {precision_rnn * 100:.2f}%\\nRecall: {recall_rnn * 100:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logical Comparison and Insights:\n",
    "#### Higher FFNN Performance: \n",
    "#### The FFNN model outperforms the RNN model across all metrics. This is a logical outcome considering the nature of the data and the task. FFNNs are well-suited for classification tasks where the input features represent independent variables, as is the case with BoW features. BoW and similar techniques like TF-IDF transform text into a vector space where the sequence of words is not preserved, aligning well with the FFNN's processing manner.\n",
    "\n",
    "#### RNN's Sequential Nature: \n",
    "#### RNNs are designed to process sequential data, capturing dependencies at different time steps. However, when applied to BoW features, the sequential aspect of the data is not utilized, which can lead to suboptimal performance. The RNN's strength in capturing the order of words or phrases is essentially nullified when the input is a BoW feature vector, explaining why it might not perform as well as the FFNN in this context.\n",
    "\n",
    "#### Precision vs. Recall: \n",
    "#### It's noteworthy that the RNN model, despite its lower overall performance, has a precision slightly higher than its recall, indicating it might be slightly more conservative in its positive predictions. However, the differences are not stark, and both models show a balanced performance between precision and recall.\n",
    "\n",
    "### Conclusions:\n",
    "#### The FFNN's superior performance for this task is logical and expected given the match between the model capabilities and the nature of the input features. FFNNs efficiently handle the independent features generated by BoW techniques, making them a strong choice for tasks involving non-sequential data.\n",
    "\n",
    "#### The RNN's lower performance highlights the importance of aligning model choice with data characteristics. For sequential data, such as raw text where the order of words carries meaning, RNNs (and their more advanced variants like LSTMs or GRUs) could outperform FFNNs. However, when the sequential information is not present or not relevant, as with BoW features, RNNs lose their advantage.\n",
    "\n",
    "#### These results underscore the principle that no single model is universally best for all tasks. The choice of model should be guided by the nature of the task, the characteristics of the data, and the specific requirements of the application.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
