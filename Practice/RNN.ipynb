{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from common.functions import evaluate_model, train_model\n",
    "from common.rnn import RNN\n",
    "from common.custom_dataset import CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>abstract</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31716</td>\n",
       "      <td>Automatic meeting analysis is an essential f...</td>\n",
       "      <td>eess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89533</td>\n",
       "      <td>We propose a protocol to encode classical bi...</td>\n",
       "      <td>quant-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82700</td>\n",
       "      <td>A number of physically intuitive results for...</td>\n",
       "      <td>quant-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78830</td>\n",
       "      <td>In the last decade rare-earth hexaborides ha...</td>\n",
       "      <td>physics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94948</td>\n",
       "      <td>We introduce the weak barycenter of a family...</td>\n",
       "      <td>stat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79995</th>\n",
       "      <td>27913</td>\n",
       "      <td>In this paper, the sum secure degrees of fre...</td>\n",
       "      <td>cs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79996</th>\n",
       "      <td>94441</td>\n",
       "      <td>In areas of application, including actuarial...</td>\n",
       "      <td>stat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79997</th>\n",
       "      <td>33015</td>\n",
       "      <td>Failure detection is employed in the industr...</td>\n",
       "      <td>eess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79998</th>\n",
       "      <td>942</td>\n",
       "      <td>As part of the ongoing effort to characteriz...</td>\n",
       "      <td>astro-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79999</th>\n",
       "      <td>67462</td>\n",
       "      <td>This paper develops an online algorithm to s...</td>\n",
       "      <td>math</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                           abstract     label\n",
       "0           31716    Automatic meeting analysis is an essential f...      eess\n",
       "1           89533    We propose a protocol to encode classical bi...  quant-ph\n",
       "2           82700    A number of physically intuitive results for...  quant-ph\n",
       "3           78830    In the last decade rare-earth hexaborides ha...   physics\n",
       "4           94948    We introduce the weak barycenter of a family...      stat\n",
       "...           ...                                                ...       ...\n",
       "79995       27913    In this paper, the sum secure degrees of fre...        cs\n",
       "79996       94441    In areas of application, including actuarial...      stat\n",
       "79997       33015    Failure detection is employed in the industr...      eess\n",
       "79998         942    As part of the ongoing effort to characteriz...  astro-ph\n",
       "79999       67462    This paper develops an online algorithm to s...      math\n",
       "\n",
       "[80000 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"../Data/arxiv_train.csv\")\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>abstract</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64481</td>\n",
       "      <td>We describe a shape derivative approach to p...</td>\n",
       "      <td>math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48104</td>\n",
       "      <td>We study displaced signatures of sneutrino p...</td>\n",
       "      <td>hep-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48233</td>\n",
       "      <td>High precision studies of Beyond-Standard-Mo...</td>\n",
       "      <td>hep-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49026</td>\n",
       "      <td>We find that a class of models of MeV-GeV da...</td>\n",
       "      <td>hep-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37957</td>\n",
       "      <td>Knowledge of power grid's topology during ca...</td>\n",
       "      <td>eess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>50391</td>\n",
       "      <td>We explore the dynamics of a simple class of...</td>\n",
       "      <td>hep-th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>63534</td>\n",
       "      <td>In this paper one construction of compositio...</td>\n",
       "      <td>math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>16712</td>\n",
       "      <td>The Random-First-Order-Transition theory of ...</td>\n",
       "      <td>cond-mat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>6596</td>\n",
       "      <td>Accurate chemical abundance measurements of ...</td>\n",
       "      <td>astro-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>93725</td>\n",
       "      <td>Generalized linear models are often misspeci...</td>\n",
       "      <td>stat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                           abstract     label\n",
       "0           64481    We describe a shape derivative approach to p...      math\n",
       "1           48104    We study displaced signatures of sneutrino p...    hep-ph\n",
       "2           48233    High precision studies of Beyond-Standard-Mo...    hep-ph\n",
       "3           49026    We find that a class of models of MeV-GeV da...    hep-ph\n",
       "4           37957    Knowledge of power grid's topology during ca...      eess\n",
       "...           ...                                                ...       ...\n",
       "19995       50391    We explore the dynamics of a simple class of...    hep-th\n",
       "19996       63534    In this paper one construction of compositio...      math\n",
       "19997       16712    The Random-First-Order-Transition theory of ...  cond-mat\n",
       "19998        6596    Accurate chemical abundance measurements of ...  astro-ph\n",
       "19999       93725    Generalized linear models are often misspeci...      stat\n",
       "\n",
       "[20000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the train set\n",
    "test_data = pd.read_csv(\"../Data/arxiv_test.csv\")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(max_features=5000)  # Adjust max_features as needed\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_count = count_vectorizer.fit_transform(train_data[\"abstract\"]).toarray()\n",
    "X_test_count = count_vectorizer.transform(test_data[\"abstract\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data[\"abstract\"]).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_data[\"abstract\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = {label: idx for idx, label in enumerate(train_data[\"label\"].unique())}\n",
    "train_data[\"label_encoded\"] = train_data[\"label\"].map(label_encoder)\n",
    "test_data[\"label_encoded\"] = test_data[\"label\"].map(label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(train_data[\"label_encoded\"].values)\n",
    "y_test = torch.tensor(test_data[\"label_encoded\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train and test datasets\n",
    "train_dataset_count = CustomDataset(X_train_count, y_train)\n",
    "test_dataset_count = CustomDataset(X_test_count, y_test)\n",
    "\n",
    "train_dataset_tfidf = CustomDataset(X_train_tfidf, y_train)\n",
    "test_dataset_tfidf = CustomDataset(X_test_tfidf, y_test)\n",
    "\n",
    "# Define data loaders\n",
    "train_loader_count = DataLoader(train_dataset_count, batch_size=64, shuffle=True)\n",
    "test_loader_count = DataLoader(test_dataset_count, batch_size=64, shuffle=False)\n",
    "\n",
    "train_loader_tfidf = DataLoader(train_dataset_tfidf, batch_size=64, shuffle=True)\n",
    "test_loader_tfidf = DataLoader(test_dataset_tfidf, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating different number of layers for Count Vectorizer feature detection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN with CountVectorizer and 1 layers\n",
      "\n",
      "Epoch [1/10], Loss: 763.3199734687805\n",
      "Epoch [2/10], Loss: 506.98329558968544\n",
      "Epoch [3/10], Loss: 400.7188519388437\n",
      "Epoch [4/10], Loss: 309.13360740989447\n",
      "Epoch [5/10], Loss: 218.14016295969486\n",
      "Epoch [6/10], Loss: 144.04848780483007\n",
      "Epoch [7/10], Loss: 90.89709524437785\n",
      "Epoch [8/10], Loss: 59.36363659566268\n",
      "Epoch [9/10], Loss: 41.46178376721218\n",
      "Epoch [10/10], Loss: 35.75288056908175\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 80.40%\n",
      "F1 Score: 80.28%\n",
      "Precision: 80.27%\n",
      "Recall: 80.35%\n",
      "\n",
      "\n",
      "Training RNN with CountVectorizer and 2 layers\n",
      "\n",
      "Epoch [1/10], Loss: 780.1336170881987\n",
      "Epoch [2/10], Loss: 533.3758004158735\n",
      "Epoch [3/10], Loss: 448.5421806126833\n",
      "Epoch [4/10], Loss: 380.02794320881367\n",
      "Epoch [5/10], Loss: 317.9964202865958\n",
      "Epoch [6/10], Loss: 259.1879898197949\n",
      "Epoch [7/10], Loss: 205.02648516744375\n",
      "Epoch [8/10], Loss: 160.85056117735803\n",
      "Epoch [9/10], Loss: 123.30341439787298\n",
      "Epoch [10/10], Loss: 95.95782375568524\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 79.81%\n",
      "F1 Score: 79.86%\n",
      "Precision: 80.00%\n",
      "Recall: 79.77%\n",
      "\n",
      "\n",
      "Training RNN with CountVectorizer and 3 layers\n",
      "\n",
      "Epoch [1/10], Loss: 823.3733338713646\n",
      "Epoch [2/10], Loss: 567.728721305728\n",
      "Epoch [3/10], Loss: 485.8672377988696\n",
      "Epoch [4/10], Loss: 428.9490951895714\n",
      "Epoch [5/10], Loss: 375.4904831945896\n",
      "Epoch [6/10], Loss: 324.01473312824965\n",
      "Epoch [7/10], Loss: 278.206056073308\n",
      "Epoch [8/10], Loss: 235.08040142059326\n",
      "Epoch [9/10], Loss: 201.31326364167035\n",
      "Epoch [10/10], Loss: 170.85590571723878\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 80.23%\n",
      "F1 Score: 80.09%\n",
      "Precision: 80.42%\n",
      "Recall: 80.22%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate RNN with CountVectorizer and different numbers of layers\n",
    "input_size_count = X_train_count.shape[1]\n",
    "output_size_count = len(label_encoder)\n",
    "for num_layers in [1, 2, 3]:\n",
    "    print(f\"Training RNN with CountVectorizer and {num_layers} layers\\n\")\n",
    "    \n",
    "    rnn_count = RNN(input_size_count, 256, output_size_count, num_layers)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(rnn_count.parameters(), lr=0.001)\n",
    "    \n",
    "    train_model(rnn_count, criterion, optimizer, train_loader_count, num_epochs=10)\n",
    "    evaluate_model(rnn_count, test_loader_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Different Activation Functions for Count Vectorizer feature detection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN with CountVectorizer and LeakyReLU as an activation function\n",
      "\n",
      "Epoch [1/10], Loss: 806.3141439259052\n",
      "Epoch [2/10], Loss: 535.044398277998\n",
      "Epoch [3/10], Loss: 451.13853193819523\n",
      "Epoch [4/10], Loss: 383.09631638228893\n",
      "Epoch [5/10], Loss: 322.45223496854305\n",
      "Epoch [6/10], Loss: 265.23423055931926\n",
      "Epoch [7/10], Loss: 211.11417189612985\n",
      "Epoch [8/10], Loss: 161.5899314135313\n",
      "Epoch [9/10], Loss: 128.69750709366053\n",
      "Epoch [10/10], Loss: 98.5733313281089\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 80.27%\n",
      "F1 Score: 80.25%\n",
      "Precision: 80.51%\n",
      "Recall: 80.26%\n",
      "\n",
      "\n",
      "Training RNN with CountVectorizer and ELU as an activation function\n",
      "\n",
      "Epoch [1/10], Loss: 785.3260654062033\n",
      "Epoch [2/10], Loss: 536.790642619133\n",
      "Epoch [3/10], Loss: 449.99404802173376\n",
      "Epoch [4/10], Loss: 382.0542240664363\n",
      "Epoch [5/10], Loss: 322.22879678756\n",
      "Epoch [6/10], Loss: 263.0231476314366\n",
      "Epoch [7/10], Loss: 208.42247082665563\n",
      "Epoch [8/10], Loss: 161.14629240706563\n",
      "Epoch [9/10], Loss: 126.28357488056645\n",
      "Epoch [10/10], Loss: 99.38592691207305\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 79.61%\n",
      "F1 Score: 79.62%\n",
      "Precision: 79.88%\n",
      "Recall: 79.54%\n",
      "\n",
      "\n",
      "Training RNN with CountVectorizer and Sigmoid as an activation function\n",
      "\n",
      "Epoch [1/10], Loss: 2064.283276438713\n",
      "Epoch [2/10], Loss: 1970.8881273269653\n",
      "Epoch [3/10], Loss: 1950.7778224945068\n",
      "Epoch [4/10], Loss: 1938.041158914566\n",
      "Epoch [5/10], Loss: 1927.8757655620575\n",
      "Epoch [6/10], Loss: 1920.8664305210114\n",
      "Epoch [7/10], Loss: 1913.7083302736282\n",
      "Epoch [8/10], Loss: 1909.107041835785\n",
      "Epoch [9/10], Loss: 1904.4576513767242\n",
      "Epoch [10/10], Loss: 1900.442488193512\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 81.97%\n",
      "F1 Score: 81.95%\n",
      "Precision: 82.08%\n",
      "Recall: 81.94%\n",
      "\n",
      "\n",
      "Training RNN with CountVectorizer and Softmax as an activation function\n",
      "\n",
      "Epoch [1/10], Loss: 2133.600352525711\n",
      "Epoch [2/10], Loss: 2045.266604423523\n",
      "Epoch [3/10], Loss: 2021.4146406650543\n",
      "Epoch [4/10], Loss: 2008.370671749115\n",
      "Epoch [5/10], Loss: 1996.970886349678\n",
      "Epoch [6/10], Loss: 1986.7596348524094\n",
      "Epoch [7/10], Loss: 1979.5033679008484\n",
      "Epoch [8/10], Loss: 1973.0980798006058\n",
      "Epoch [9/10], Loss: 1967.1504536867142\n",
      "Epoch [10/10], Loss: 1961.8876105546951\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 81.51%\n",
      "F1 Score: 81.40%\n",
      "Precision: 81.56%\n",
      "Recall: 81.49%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate FFNN with CountVectorizer and different activation function\n",
    "input_size_count = X_train_count.shape[1]\n",
    "output_size_count = len(label_encoder)\n",
    "activation_functions = [nn.LeakyReLU(), nn.ELU(), nn.Sigmoid(), nn.Softmax(dim=1)]\n",
    "\n",
    "for activation_function in activation_functions:\n",
    "    print(f\"Training RNN with CountVectorizer and {activation_function.__class__.__name__} as an activation function\\n\")\n",
    "    \n",
    "    rnn_count = RNN(input_size_count, 256, output_size_count, num_layers=2, activation=activation_function)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(rnn_count.parameters(), lr=0.001)\n",
    "    \n",
    "    train_model(rnn_count, criterion, optimizer, train_loader_count, num_epochs=10)\n",
    "    evaluate_model(rnn_count, test_loader_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating different number of layers for tfidf feature detection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN with TF-IDF and 1 layers\n",
      "\n",
      "Epoch [1/10], Loss: 974.9493601024151\n",
      "Epoch [2/10], Loss: 560.0621326118708\n",
      "Epoch [3/10], Loss: 495.56233832240105\n",
      "Epoch [4/10], Loss: 459.116337954998\n",
      "Epoch [5/10], Loss: 432.02958865463734\n",
      "Epoch [6/10], Loss: 412.2890088185668\n",
      "Epoch [7/10], Loss: 397.56641874462366\n",
      "Epoch [8/10], Loss: 384.2523663341999\n",
      "Epoch [9/10], Loss: 375.58368457853794\n",
      "Epoch [10/10], Loss: 364.91814502328634\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 79.35%\n",
      "F1 Score: 79.27%\n",
      "Precision: 79.29%\n",
      "Recall: 79.32%\n",
      "\n",
      "\n",
      "Training RNN with TF-IDF and 2 layers\n",
      "\n",
      "Epoch [1/10], Loss: 944.0967095196247\n",
      "Epoch [2/10], Loss: 586.5105360001326\n",
      "Epoch [3/10], Loss: 524.6750490069389\n",
      "Epoch [4/10], Loss: 484.3246194422245\n",
      "Epoch [5/10], Loss: 457.8491253107786\n",
      "Epoch [6/10], Loss: 432.71689858287573\n",
      "Epoch [7/10], Loss: 417.4559953585267\n",
      "Epoch [8/10], Loss: 405.78441286087036\n",
      "Epoch [9/10], Loss: 391.60116408765316\n",
      "Epoch [10/10], Loss: 376.27822683006525\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 79.81%\n",
      "F1 Score: 79.65%\n",
      "Precision: 79.93%\n",
      "Recall: 79.79%\n",
      "\n",
      "\n",
      "Training RNN with TF-IDF and 3 layers\n",
      "\n",
      "Epoch [1/10], Loss: 1007.2768024802208\n",
      "Epoch [2/10], Loss: 671.6151727288961\n",
      "Epoch [3/10], Loss: 600.7300136238337\n",
      "Epoch [4/10], Loss: 559.8084570169449\n",
      "Epoch [5/10], Loss: 527.43466553092\n",
      "Epoch [6/10], Loss: 505.0861467421055\n",
      "Epoch [7/10], Loss: 487.72477073967457\n",
      "Epoch [8/10], Loss: 477.34231838583946\n",
      "Epoch [9/10], Loss: 466.30622360110283\n",
      "Epoch [10/10], Loss: 455.69355918467045\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 79.69%\n",
      "F1 Score: 79.42%\n",
      "Precision: 79.44%\n",
      "Recall: 79.65%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_size_tfidf = X_train_tfidf.shape[1]\n",
    "output_size_tfidf = len(label_encoder)\n",
    "for num_layers in [1, 2, 3]:\n",
    "    print(f\"Training RNN with TF-IDF and {num_layers} layers\\n\")\n",
    "    \n",
    "    rnn_tfidf = RNN(input_size_tfidf, 256, output_size_tfidf, num_layers)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(rnn_tfidf.parameters(), lr=0.001)\n",
    "    \n",
    "    train_model(rnn_tfidf, criterion, optimizer, train_loader_tfidf, num_epochs=10)\n",
    "    evaluate_model(rnn_tfidf, test_loader_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Activation Function for tfidf feature detection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN with tfidf and LeakyReLU as an activation function\n",
      "\n",
      "Epoch [1/10], Loss: 959.5855261832476\n",
      "Epoch [2/10], Loss: 586.7226179987192\n",
      "Epoch [3/10], Loss: 520.5368458628654\n",
      "Epoch [4/10], Loss: 482.04027332365513\n",
      "Epoch [5/10], Loss: 456.0838768184185\n",
      "Epoch [6/10], Loss: 435.62667295336723\n",
      "Epoch [7/10], Loss: 418.85956501215696\n",
      "Epoch [8/10], Loss: 405.2117108106613\n",
      "Epoch [9/10], Loss: 389.3891863003373\n",
      "Epoch [10/10], Loss: 380.1468401700258\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 79.97%\n",
      "F1 Score: 79.87%\n",
      "Precision: 79.95%\n",
      "Recall: 79.90%\n",
      "\n",
      "\n",
      "Training RNN with tfidf and ELU as an activation function\n",
      "\n",
      "Epoch [1/10], Loss: 930.935423463583\n",
      "Epoch [2/10], Loss: 590.0360115617514\n",
      "Epoch [3/10], Loss: 524.5577806830406\n",
      "Epoch [4/10], Loss: 484.2687980681658\n",
      "Epoch [5/10], Loss: 459.3589107468724\n",
      "Epoch [6/10], Loss: 436.2060491144657\n",
      "Epoch [7/10], Loss: 417.8359133377671\n",
      "Epoch [8/10], Loss: 405.26593001931906\n",
      "Epoch [9/10], Loss: 392.54777332395315\n",
      "Epoch [10/10], Loss: 378.93930273503065\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 80.06%\n",
      "F1 Score: 80.05%\n",
      "Precision: 80.13%\n",
      "Recall: 80.03%\n",
      "\n",
      "\n",
      "Training RNN with tfidf and Sigmoid as an activation function\n",
      "\n",
      "Epoch [1/10], Loss: 962.5447880327702\n",
      "Epoch [2/10], Loss: 606.4774402827024\n",
      "Epoch [3/10], Loss: 534.8805330097675\n",
      "Epoch [4/10], Loss: 491.0621878057718\n",
      "Epoch [5/10], Loss: 461.1515766978264\n",
      "Epoch [6/10], Loss: 441.98460668325424\n",
      "Epoch [7/10], Loss: 422.20350881665945\n",
      "Epoch [8/10], Loss: 405.80147302895784\n",
      "Epoch [9/10], Loss: 392.492348279804\n",
      "Epoch [10/10], Loss: 382.2981627434492\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 80.17%\n",
      "F1 Score: 80.05%\n",
      "Precision: 80.13%\n",
      "Recall: 80.13%\n",
      "\n",
      "\n",
      "Training RNN with tfidf and Softmax as an activation function\n",
      "\n",
      "Epoch [1/10], Loss: 927.5247004032135\n",
      "Epoch [2/10], Loss: 592.3291422128677\n",
      "Epoch [3/10], Loss: 524.2254588454962\n",
      "Epoch [4/10], Loss: 486.63520750403404\n",
      "Epoch [5/10], Loss: 457.44610779732466\n",
      "Epoch [6/10], Loss: 433.9056380689144\n",
      "Epoch [7/10], Loss: 417.65942650288343\n",
      "Epoch [8/10], Loss: 404.454608514905\n",
      "Epoch [9/10], Loss: 390.4108237400651\n",
      "Epoch [10/10], Loss: 379.1731286495924\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 79.61%\n",
      "F1 Score: 79.51%\n",
      "Precision: 79.57%\n",
      "Recall: 79.59%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_size_tfidf = X_train_tfidf.shape[1]\n",
    "output_size_tfidf = len(label_encoder)\n",
    "activation_functions = [nn.LeakyReLU(), nn.ELU(), nn.Sigmoid(), nn.Softmax(dim=1)]\n",
    "\n",
    "for activation_function in activation_functions:\n",
    "    print(f\"Training RNN with tfidf and {activation_function.__class__.__name__} as an activation function\\n\")\n",
    "    \n",
    "    rnn_tfidf = RNN(input_size_tfidf, 256, output_size_tfidf, num_layers=2)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(rnn_tfidf.parameters(), lr=0.001)\n",
    "    \n",
    "    train_model(rnn_tfidf, criterion, optimizer, train_loader_tfidf, num_epochs=10)\n",
    "    evaluate_model(rnn_tfidf, test_loader_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating hidden size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FFNN_hidden_size(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size, hidden_sizes, output_size, activation=nn.ReLU()\n",
    "    ):\n",
    "        super(FFNN_hidden_size, self).__init__()  # Corrected super call\n",
    "        self.num_layers = len(hidden_sizes) + 1\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(self.num_layers - 1):\n",
    "            if i == 0:\n",
    "                self.hidden_layers.append(nn.Linear(input_size, hidden_sizes[i]))\n",
    "            else:\n",
    "                self.hidden_layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))\n",
    "            self.hidden_layers.append(activation)\n",
    "        self.hidden_layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating hidden size for Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FFNN with CountVectorizer and 64 as hidden size\n",
      "\n",
      "Epoch [1/10], Loss: 928.1101182699203\n",
      "Epoch [2/10], Loss: 521.2363376766443\n",
      "Epoch [3/10], Loss: 431.4762702435255\n",
      "Epoch [4/10], Loss: 367.9116633683443\n",
      "Epoch [5/10], Loss: 313.4780855104327\n",
      "Epoch [6/10], Loss: 265.27464877814054\n",
      "Epoch [7/10], Loss: 220.85574132576585\n",
      "Epoch [8/10], Loss: 179.75636037439108\n",
      "Epoch [9/10], Loss: 142.45119661837816\n",
      "Epoch [10/10], Loss: 109.49029067344964\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 81.19%\n",
      "F1 Score: 81.05%\n",
      "Precision: 81.11%\n",
      "Recall: 81.15%\n",
      "\n",
      "\n",
      "Training FFNN with CountVectorizer and 128 as hidden size\n",
      "\n",
      "Epoch [1/10], Loss: 853.1412792801857\n",
      "Epoch [2/10], Loss: 506.3931464701891\n",
      "Epoch [3/10], Loss: 415.78018732368946\n",
      "Epoch [4/10], Loss: 347.91063272953033\n",
      "Epoch [5/10], Loss: 285.9519297629595\n",
      "Epoch [6/10], Loss: 230.24208783730865\n",
      "Epoch [7/10], Loss: 176.87036663293839\n",
      "Epoch [8/10], Loss: 130.54939570464194\n",
      "Epoch [9/10], Loss: 90.7079890165478\n",
      "Epoch [10/10], Loss: 59.17798920162022\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 80.69%\n",
      "F1 Score: 80.69%\n",
      "Precision: 80.76%\n",
      "Recall: 80.65%\n",
      "\n",
      "\n",
      "Training FFNN with CountVectorizer and 256 as hidden size\n",
      "\n",
      "Epoch [1/10], Loss: 801.0472709685564\n",
      "Epoch [2/10], Loss: 497.62703162431717\n",
      "Epoch [3/10], Loss: 405.3043258935213\n",
      "Epoch [4/10], Loss: 329.54894000291824\n",
      "Epoch [5/10], Loss: 260.2215184085071\n",
      "Epoch [6/10], Loss: 193.69686809182167\n",
      "Epoch [7/10], Loss: 132.5204858519137\n",
      "Epoch [8/10], Loss: 83.83092058263719\n",
      "Epoch [9/10], Loss: 47.3569332966581\n",
      "Epoch [10/10], Loss: 25.08842640556395\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 80.54%\n",
      "F1 Score: 80.49%\n",
      "Precision: 80.59%\n",
      "Recall: 80.49%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_size_count = X_train_count.shape[1]\n",
    "output_size_count = len(label_encoder)\n",
    "hidden_sizes = [64, 128, 256]\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    print(f\"Training FFNN with CountVectorizer and {hidden_size} as hidden size\\n\")\n",
    "    \n",
    "    ffnn_count = FFNN_hidden_size(input_size_count, [hidden_size], output_size_count, activation=nn.Sigmoid())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(ffnn_count.parameters(), lr=0.001)\n",
    "    \n",
    "    train_model(ffnn_count, criterion, optimizer, train_loader_count, num_epochs=10)\n",
    "    evaluate_model(ffnn_count, test_loader_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for evaluating batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with batch size: 32\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 69.58%\n",
      "F1 Score: 63.04%\n",
      "Precision: 59.33%\n",
      "Recall: 69.50%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 69.14%\n",
      "F1 Score: 63.37%\n",
      "Precision: 60.27%\n",
      "Recall: 69.05%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 68.45%\n",
      "F1 Score: 62.82%\n",
      "Precision: 59.68%\n",
      "Recall: 68.36%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 68.44%\n",
      "F1 Score: 62.67%\n",
      "Precision: 59.25%\n",
      "Recall: 68.36%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 67.90%\n",
      "F1 Score: 62.61%\n",
      "Precision: 60.48%\n",
      "Recall: 67.84%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 68.08%\n",
      "F1 Score: 62.53%\n",
      "Precision: 59.59%\n",
      "Recall: 68.00%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 67.51%\n",
      "F1 Score: 62.35%\n",
      "Precision: 59.82%\n",
      "Recall: 67.43%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 67.19%\n",
      "F1 Score: 62.11%\n",
      "Precision: 59.64%\n",
      "Recall: 67.11%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 67.50%\n",
      "F1 Score: 62.37%\n",
      "Precision: 59.92%\n",
      "Recall: 67.42%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 67.42%\n",
      "F1 Score: 62.33%\n",
      "Precision: 59.80%\n",
      "Recall: 67.34%\n",
      "\n",
      "\n",
      "Accuracy with batch size 256: None\n",
      "Evaluating with batch size: 64\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 77.80%\n",
      "F1 Score: 73.94%\n",
      "Precision: 71.02%\n",
      "Recall: 77.65%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 77.68%\n",
      "F1 Score: 73.80%\n",
      "Precision: 71.06%\n",
      "Recall: 77.55%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 77.22%\n",
      "F1 Score: 73.59%\n",
      "Precision: 71.23%\n",
      "Recall: 77.10%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 76.81%\n",
      "F1 Score: 73.28%\n",
      "Precision: 70.81%\n",
      "Recall: 76.69%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 76.66%\n",
      "F1 Score: 72.92%\n",
      "Precision: 70.33%\n",
      "Recall: 76.54%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 75.97%\n",
      "F1 Score: 72.51%\n",
      "Precision: 70.29%\n",
      "Recall: 75.86%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 75.95%\n",
      "F1 Score: 72.46%\n",
      "Precision: 70.01%\n",
      "Recall: 75.83%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 76.11%\n",
      "F1 Score: 72.63%\n",
      "Precision: 70.27%\n",
      "Recall: 75.98%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 76.07%\n",
      "F1 Score: 72.64%\n",
      "Precision: 70.39%\n",
      "Recall: 75.95%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 76.02%\n",
      "F1 Score: 72.59%\n",
      "Precision: 70.34%\n",
      "Recall: 75.90%\n",
      "\n",
      "\n",
      "Accuracy with batch size 256: None\n",
      "Evaluating with batch size: 128\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 72.34%\n",
      "F1 Score: 65.22%\n",
      "Precision: 60.64%\n",
      "Recall: 72.24%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 72.17%\n",
      "F1 Score: 65.36%\n",
      "Precision: 61.17%\n",
      "Recall: 72.07%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 71.54%\n",
      "F1 Score: 64.94%\n",
      "Precision: 60.99%\n",
      "Recall: 71.45%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 71.68%\n",
      "F1 Score: 65.18%\n",
      "Precision: 61.38%\n",
      "Recall: 71.58%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 71.39%\n",
      "F1 Score: 65.02%\n",
      "Precision: 61.39%\n",
      "Recall: 71.28%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 71.26%\n",
      "F1 Score: 64.92%\n",
      "Precision: 61.27%\n",
      "Recall: 71.16%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 70.81%\n",
      "F1 Score: 64.86%\n",
      "Precision: 61.79%\n",
      "Recall: 70.72%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 70.84%\n",
      "F1 Score: 64.88%\n",
      "Precision: 61.79%\n",
      "Recall: 70.74%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 70.49%\n",
      "F1 Score: 64.66%\n",
      "Precision: 61.74%\n",
      "Recall: 70.41%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 70.58%\n",
      "F1 Score: 64.74%\n",
      "Precision: 61.78%\n",
      "Recall: 70.49%\n",
      "\n",
      "\n",
      "Accuracy with batch size 256: None\n",
      "Evaluating with batch size: 256\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 37.63%\n",
      "F1 Score: 26.62%\n",
      "Precision: 22.61%\n",
      "Recall: 37.61%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 46.69%\n",
      "F1 Score: 36.13%\n",
      "Precision: 31.75%\n",
      "Recall: 46.62%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 46.66%\n",
      "F1 Score: 36.65%\n",
      "Precision: 32.80%\n",
      "Recall: 46.59%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 46.44%\n",
      "F1 Score: 36.54%\n",
      "Precision: 32.70%\n",
      "Recall: 46.38%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 46.26%\n",
      "F1 Score: 36.74%\n",
      "Precision: 33.30%\n",
      "Recall: 46.19%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 45.96%\n",
      "F1 Score: 36.82%\n",
      "Precision: 33.66%\n",
      "Recall: 45.90%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 45.93%\n",
      "F1 Score: 36.97%\n",
      "Precision: 33.97%\n",
      "Recall: 45.87%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 45.62%\n",
      "F1 Score: 37.06%\n",
      "Precision: 34.43%\n",
      "Recall: 45.57%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 45.66%\n",
      "F1 Score: 37.06%\n",
      "Precision: 34.39%\n",
      "Recall: 45.61%\n",
      "\n",
      "\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 45.73%\n",
      "F1 Score: 37.07%\n",
      "Precision: 34.35%\n",
      "Recall: 45.67%\n",
      "\n",
      "\n",
      "Accuracy with batch size 256: None\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "batch_sizes = [32, 64, 128, 256] \n",
    "results = {}\n",
    "input_size_count = X_train_count.shape[1]\n",
    "output_size_count = len(label_encoder)\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"Evaluating with batch size: {batch_size}\")\n",
    "    \n",
    "    model = FFNN(input_size_count, 256, output_size_count, num_layers=2, activation=nn.ReLU())\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for batch_size in batch_sizes: \n",
    "        train_loader_count = DataLoader(train_dataset_count, batch_size, shuffle=True)\n",
    "        test_loader_count = DataLoader(test_dataset_count, batch_size, shuffle=False)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader_count:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.float()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        accuracy = evaluate_model(model, test_loader_count)\n",
    "    results[batch_size] = accuracy\n",
    "    print(f\"Accuracy with batch size {batch_size}: {accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results:\n",
      "Batch size: 256, Accuracy: None\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation results:\")\n",
    "for batch_size, accuracy in results.items():\n",
    "    print(f\"Batch size: {batch_size}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FFNN_regularization(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.5):\n",
    "        super(FFNN_regularization, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNN regularization\n",
      "\n",
      "Epoch [1/10], Loss: 222.71560445427895\n",
      "Epoch [2/10], Loss: 141.26893106102943\n",
      "Epoch [3/10], Loss: 114.34151920676231\n",
      "Epoch [4/10], Loss: 94.36796480417252\n",
      "Epoch [5/10], Loss: 76.94665214419365\n",
      "Epoch [6/10], Loss: 62.584375970065594\n",
      "Epoch [7/10], Loss: 49.860425889492035\n",
      "Epoch [8/10], Loss: 40.20941513776779\n",
      "Epoch [9/10], Loss: 33.51778922602534\n",
      "Epoch [10/10], Loss: 27.590626560151577\n",
      "\n",
      "Evaluations:\n",
      "Accuracy: 82.67%\n",
      "F1 Score: 82.60%\n",
      "Precision: 82.65%\n",
      "Recall: 82.65%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_size_count = X_train_count.shape[1]\n",
    "output_size_count = len(label_encoder)\n",
    "print(f\"FFNN regularization\\n\")\n",
    "\n",
    "ffnn_count = FFNN_regularization(input_size_count, 256, output_size_count)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(ffnn_count.parameters(), lr=0.001)\n",
    "\n",
    "train_model(ffnn_count, criterion, optimizer, train_loader_count, num_epochs=10)\n",
    "evaluate_model(ffnn_count, test_loader_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
