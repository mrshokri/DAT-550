{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>abstract</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31716</td>\n",
       "      <td>Automatic meeting analysis is an essential f...</td>\n",
       "      <td>eess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89533</td>\n",
       "      <td>We propose a protocol to encode classical bi...</td>\n",
       "      <td>quant-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82700</td>\n",
       "      <td>A number of physically intuitive results for...</td>\n",
       "      <td>quant-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78830</td>\n",
       "      <td>In the last decade rare-earth hexaborides ha...</td>\n",
       "      <td>physics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94948</td>\n",
       "      <td>We introduce the weak barycenter of a family...</td>\n",
       "      <td>stat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79995</th>\n",
       "      <td>27913</td>\n",
       "      <td>In this paper, the sum secure degrees of fre...</td>\n",
       "      <td>cs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79996</th>\n",
       "      <td>94441</td>\n",
       "      <td>In areas of application, including actuarial...</td>\n",
       "      <td>stat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79997</th>\n",
       "      <td>33015</td>\n",
       "      <td>Failure detection is employed in the industr...</td>\n",
       "      <td>eess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79998</th>\n",
       "      <td>942</td>\n",
       "      <td>As part of the ongoing effort to characteriz...</td>\n",
       "      <td>astro-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79999</th>\n",
       "      <td>67462</td>\n",
       "      <td>This paper develops an online algorithm to s...</td>\n",
       "      <td>math</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                           abstract     label\n",
       "0           31716    Automatic meeting analysis is an essential f...      eess\n",
       "1           89533    We propose a protocol to encode classical bi...  quant-ph\n",
       "2           82700    A number of physically intuitive results for...  quant-ph\n",
       "3           78830    In the last decade rare-earth hexaborides ha...   physics\n",
       "4           94948    We introduce the weak barycenter of a family...      stat\n",
       "...           ...                                                ...       ...\n",
       "79995       27913    In this paper, the sum secure degrees of fre...        cs\n",
       "79996       94441    In areas of application, including actuarial...      stat\n",
       "79997       33015    Failure detection is employed in the industr...      eess\n",
       "79998         942    As part of the ongoing effort to characteriz...  astro-ph\n",
       "79999       67462    This paper develops an online algorithm to s...      math\n",
       "\n",
       "[80000 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"./arxiv_train.csv\")\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>abstract</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64481</td>\n",
       "      <td>We describe a shape derivative approach to p...</td>\n",
       "      <td>math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48104</td>\n",
       "      <td>We study displaced signatures of sneutrino p...</td>\n",
       "      <td>hep-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48233</td>\n",
       "      <td>High precision studies of Beyond-Standard-Mo...</td>\n",
       "      <td>hep-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49026</td>\n",
       "      <td>We find that a class of models of MeV-GeV da...</td>\n",
       "      <td>hep-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37957</td>\n",
       "      <td>Knowledge of power grid's topology during ca...</td>\n",
       "      <td>eess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>50391</td>\n",
       "      <td>We explore the dynamics of a simple class of...</td>\n",
       "      <td>hep-th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>63534</td>\n",
       "      <td>In this paper one construction of compositio...</td>\n",
       "      <td>math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>16712</td>\n",
       "      <td>The Random-First-Order-Transition theory of ...</td>\n",
       "      <td>cond-mat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>6596</td>\n",
       "      <td>Accurate chemical abundance measurements of ...</td>\n",
       "      <td>astro-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>93725</td>\n",
       "      <td>Generalized linear models are often misspeci...</td>\n",
       "      <td>stat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                           abstract     label\n",
       "0           64481    We describe a shape derivative approach to p...      math\n",
       "1           48104    We study displaced signatures of sneutrino p...    hep-ph\n",
       "2           48233    High precision studies of Beyond-Standard-Mo...    hep-ph\n",
       "3           49026    We find that a class of models of MeV-GeV da...    hep-ph\n",
       "4           37957    Knowledge of power grid's topology during ca...      eess\n",
       "...           ...                                                ...       ...\n",
       "19995       50391    We explore the dynamics of a simple class of...    hep-th\n",
       "19996       63534    In this paper one construction of compositio...      math\n",
       "19997       16712    The Random-First-Order-Transition theory of ...  cond-mat\n",
       "19998        6596    Accurate chemical abundance measurements of ...  astro-ph\n",
       "19999       93725    Generalized linear models are often misspeci...      stat\n",
       "\n",
       "[20000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the train set\n",
    "test_data = pd.read_csv(\"./arxiv_test.csv\")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\SaharKadkhodaMasoumA/nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\\\share\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\SaharKadkhodaMasoumA\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\SaharKadkhodaMasoumA/nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\\\share\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\SaharKadkhodaMasoumA\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m     clean_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens)  \u001b[38;5;66;03m# Join tokens back into a single string\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clean_text\n\u001b[1;32m---> 14\u001b[0m train_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_abstract\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mabstract\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m test_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_abstract\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m test_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(clean_text)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\series.py:4915\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4781\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4782\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4788\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4791\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4906\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4907\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4909\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4913\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4915\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m, in \u001b[0;36mclean_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      5\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[0;32m      6\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()  \u001b[38;5;66;03m# Lemmatize each word\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[0;32m      8\u001b[0m stemmer \u001b[38;5;241m=\u001b[39m PorterStemmer()  \u001b[38;5;66;03m# Stem each word\u001b[39;00m\n\u001b[0;32m      9\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [stemmer\u001b[38;5;241m.\u001b[39mstem(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      5\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[0;32m      6\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()  \u001b[38;5;66;03m# Lemmatize each word\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [\u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[0;32m      8\u001b[0m stemmer \u001b[38;5;241m=\u001b[39m PorterStemmer()  \u001b[38;5;66;03m# Stem each word\u001b[39;00m\n\u001b[0;32m      9\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [stemmer\u001b[38;5;241m.\u001b[39mstem(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\stem\\wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m(word, pos)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\SaharKadkhodaMasoumA/nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\\\share\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\SaharKadkhodaMasoumA\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove special characters and numbers\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize the text\n",
    "    stop_words = set(stopwords.words(\"english\"))  # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()  # Lemmatize each word\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    stemmer = PorterStemmer()  # Stem each word\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    clean_text = \" \".join(tokens)  # Join tokens back into a single string\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "train_data[\"clean_abstract\"] = train_data[\"abstract\"].apply(clean_text)\n",
    "test_data[\"clean_abstract\"] = test_data[\"abstract\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(max_features=5000)  # Adjust max_features as needed\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_count = count_vectorizer.fit_transform(train_data[\"abstract\"]).toarray()\n",
    "X_test_count = count_vectorizer.transform(test_data[\"abstract\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data[\"abstract\"]).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_data[\"abstract\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = {label: idx for idx, label in enumerate(train_data[\"label\"].unique())}\n",
    "train_data[\"label_encoded\"] = train_data[\"label\"].map(label_encoder)\n",
    "test_data[\"label_encoded\"] = test_data[\"label\"].map(label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(train_data[\"label_encoded\"].values)\n",
    "y_test = torch.tensor(test_data[\"label_encoded\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class FFNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [nn.Linear(hidden_size, hidden_size) for _ in range(num_layers - 1)]\n",
    "        )\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        for layer in self.hidden_layers:\n",
    "            x = torch.relu(layer(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, activation=nn.ReLU()):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                self.hidden_layers.append(nn.Linear(input_size, hidden_size))\n",
    "            elif i == num_layers - 1:\n",
    "                self.hidden_layers.append(nn.Linear(hidden_size, output_size))\n",
    "            else:\n",
    "                self.hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.hidden_layers.append(activation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train_model(model, criterion, optimizer, train_loader, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.float())\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs.float())\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train and test datasets\n",
    "train_dataset_count = CustomDataset(X_train_count, y_train)\n",
    "test_dataset_count = CustomDataset(X_test_count, y_test)\n",
    "\n",
    "train_dataset_tfidf = CustomDataset(X_train_tfidf, y_train)\n",
    "test_dataset_tfidf = CustomDataset(X_test_tfidf, y_test)\n",
    "\n",
    "# Define data loaders\n",
    "train_loader_count = DataLoader(train_dataset_count, batch_size=64, shuffle=True)\n",
    "test_loader_count = DataLoader(test_dataset_count, batch_size=64, shuffle=False)\n",
    "\n",
    "train_loader_tfidf = DataLoader(train_dataset_tfidf, batch_size=64, shuffle=True)\n",
    "test_loader_tfidf = DataLoader(test_dataset_tfidf, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FFNN with CountVectorizer and 1 layers\n",
      "Epoch [1/10], Loss: 743.0623899400234\n",
      "Epoch [2/10], Loss: 445.8628660440445\n",
      "Epoch [3/10], Loss: 263.4596190750599\n",
      "Epoch [4/10], Loss: 114.98958802036941\n",
      "Epoch [5/10], Loss: 38.809479392599314\n",
      "Epoch [6/10], Loss: 14.333205292350613\n",
      "Epoch [7/10], Loss: 7.945223589427769\n",
      "Epoch [8/10], Loss: 20.164672709885053\n",
      "Epoch [9/10], Loss: 16.07088966111769\n",
      "Epoch [10/10], Loss: 7.142239848835743\n",
      "Evaluation on Test Set:\n",
      "Accuracy: 0.8107, Precision: 0.8109409515555879, Recall: 0.8104510494535951, F1-Score: 0.8096136114078154\n",
      "\n",
      "Training FFNN with CountVectorizer and 2 layers\n",
      "Epoch [1/10], Loss: 749.5075382143259\n",
      "Epoch [2/10], Loss: 440.41998664289713\n",
      "Epoch [3/10], Loss: 258.40501895174384\n",
      "Epoch [4/10], Loss: 110.69346080627292\n",
      "Epoch [5/10], Loss: 52.09413174400106\n",
      "Epoch [6/10], Loss: 48.67403827956878\n",
      "Epoch [7/10], Loss: 33.54433995686122\n",
      "Epoch [8/10], Loss: 30.207818542796304\n",
      "Epoch [9/10], Loss: 32.536002864377224\n",
      "Epoch [10/10], Loss: 25.806746830567135\n",
      "Evaluation on Test Set:\n",
      "Accuracy: 0.806, Precision: 0.8069247174341326, Recall: 0.8057463806897079, F1-Score: 0.8056152564440584\n",
      "\n",
      "Training FFNN with CountVectorizer and 3 layers\n",
      "Epoch [1/10], Loss: 769.404712587595\n",
      "Epoch [2/10], Loss: 459.93614304065704\n",
      "Epoch [3/10], Loss: 273.43068661913276\n",
      "Epoch [4/10], Loss: 140.1108162254095\n",
      "Epoch [5/10], Loss: 73.98355375614483\n",
      "Epoch [6/10], Loss: 55.99484885338461\n",
      "Epoch [7/10], Loss: 42.89699421336991\n",
      "Epoch [8/10], Loss: 36.55461804187871\n",
      "Epoch [9/10], Loss: 34.396219057591225\n",
      "Epoch [10/10], Loss: 28.933283245394705\n",
      "Evaluation on Test Set:\n",
      "Accuracy: 0.8055, Precision: 0.8052012348786637, Recall: 0.8052588610874144, F1-Score: 0.8040077378305313\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate FFNN with CountVectorizer and different numbers of layers\n",
    "input_size_count = X_train_count.shape[1]\n",
    "output_size_count = len(label_encoder)\n",
    "for num_layers in [1, 2, 3]:\n",
    "    print(f\"Training FFNN with CountVectorizer and {num_layers} layers\")\n",
    "    ffnn_count = FFNN(input_size_count, 256, output_size_count, num_layers)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(ffnn_count.parameters(), lr=0.001)\n",
    "    train_model(ffnn_count, criterion, optimizer, train_loader_count, num_epochs=10)\n",
    "    print(\"Evaluation on Test Set:\")\n",
    "    evaluate_model(ffnn_count, test_loader_count)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FFNN with CountVectorizer and LeakyReLU as an activation function\n",
      "Epoch [1/10], Loss: 779.1226999759674\n",
      "Epoch [2/10], Loss: 461.2558950930834\n",
      "Epoch [3/10], Loss: 290.4565050601959\n",
      "Epoch [4/10], Loss: 142.1204579025507\n",
      "Epoch [5/10], Loss: 54.91493967361748\n",
      "Epoch [6/10], Loss: 22.617049310123548\n",
      "Epoch [7/10], Loss: 17.038059719197918\n",
      "Epoch [8/10], Loss: 20.81000182213029\n",
      "Epoch [9/10], Loss: 12.523910178308142\n",
      "Epoch [10/10], Loss: 10.43505616929906\n",
      "Evaluation on Test Set:\n",
      "Accuracy: 0.8091, Precision: 0.8084935102365746, Recall: 0.8085558081893665, F1-Score: 0.8082112300010884\n",
      "\n",
      "Training FFNN with CountVectorizer and ELU as an activation function\n",
      "Epoch [1/10], Loss: 733.471880197525\n",
      "Epoch [2/10], Loss: 483.33389146625996\n",
      "Epoch [3/10], Loss: 373.9589847549796\n",
      "Epoch [4/10], Loss: 272.5039286017418\n",
      "Epoch [5/10], Loss: 177.92759406752884\n",
      "Epoch [6/10], Loss: 97.2131339032203\n",
      "Epoch [7/10], Loss: 48.75164867145941\n",
      "Epoch [8/10], Loss: 25.524274023831822\n",
      "Epoch [9/10], Loss: 18.646939551923424\n",
      "Epoch [10/10], Loss: 13.896223361720331\n",
      "Evaluation on Test Set:\n",
      "Accuracy: 0.80465, Precision: 0.8040727404852188, Recall: 0.8042485883732702, F1-Score: 0.8035880516508642\n",
      "\n",
      "Training FFNN with CountVectorizer and Sigmoid as an activation function\n",
      "Epoch [1/10], Loss: 2110.6719179153442\n",
      "Epoch [2/10], Loss: 1972.3540918827057\n",
      "Epoch [3/10], Loss: 1943.1993063688278\n",
      "Epoch [4/10], Loss: 1924.9959646463394\n",
      "Epoch [5/10], Loss: 1911.9468610286713\n",
      "Epoch [6/10], Loss: 1902.2149419784546\n",
      "Epoch [7/10], Loss: 1895.0618300437927\n",
      "Epoch [8/10], Loss: 1889.2772533893585\n",
      "Epoch [9/10], Loss: 1884.8527474403381\n",
      "Epoch [10/10], Loss: 1881.1519565582275\n",
      "Evaluation on Test Set:\n",
      "Accuracy: 0.82655, Precision: 0.8255800498100377, Recall: 0.8261729584203765, F1-Score: 0.8254171529063544\n",
      "\n",
      "Training FFNN with CountVectorizer and Softmax as an activation function\n",
      "Epoch [1/10], Loss: 2686.5611946582794\n",
      "Epoch [2/10], Loss: 2431.937489748001\n",
      "Epoch [3/10], Loss: 2370.8911254405975\n",
      "Epoch [4/10], Loss: 2349.3485156297684\n",
      "Epoch [5/10], Loss: 2336.2262196540833\n",
      "Epoch [6/10], Loss: 2323.2664573192596\n",
      "Epoch [7/10], Loss: 2309.233703494072\n",
      "Epoch [8/10], Loss: 2292.3268373012543\n",
      "Epoch [9/10], Loss: 2274.1806730031967\n",
      "Epoch [10/10], Loss: 2254.2563601732254\n",
      "Evaluation on Test Set:\n",
      "Accuracy: 0.62685, Precision: 0.6271201022372678, Recall: 0.6288295173172381, F1-Score: 0.5785536079913695\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate FFNN with CountVectorizer and different activation function\n",
    "input_size_count = X_train_count.shape[1]\n",
    "output_size_count = len(label_encoder)\n",
    "activation_functions = [nn.LeakyReLU(), nn.ELU(), nn.Sigmoid(), nn.Softmax(dim=1)]\n",
    "\n",
    "for activation_function in activation_functions:\n",
    "    print(f\"Training FFNN with CountVectorizer and {activation_function.__class__.__name__} as an activation function\")\n",
    "    ffnn_count = FFNN(input_size_count, 256, output_size_count, num_layers=2, activation=activation_function)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(ffnn_count.parameters(), lr=0.001)\n",
    "    train_model(ffnn_count, criterion, optimizer, train_loader_count, num_epochs=10)\n",
    "    print(\"Evaluation on Test Set:\")\n",
    "    evaluate_model(ffnn_count, test_loader_count)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FFNN with TF-IDF and 1 layers\n",
      "Epoch [1/10], Loss: 4564.730442523956\n",
      "Epoch [2/10], Loss: 2367.812207341194\n",
      "Epoch [3/10], Loss: 1836.384837269783\n",
      "Epoch [4/10], Loss: 1624.8815511465073\n",
      "Epoch [5/10], Loss: 1509.1928326487541\n",
      "Epoch [6/10], Loss: 1435.4432886242867\n",
      "Epoch [7/10], Loss: 1383.907634139061\n",
      "Epoch [8/10], Loss: 1345.6823671460152\n",
      "Epoch [9/10], Loss: 1315.9424677491188\n",
      "Epoch [10/10], Loss: 1291.9441948533058\n",
      "Evaluation on Test Set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SaharKadkhodaMasoumA\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.74615, Precision: 0.6750732653302313, Recall: 0.7452886692498367, F1-Score: 0.7071678329751698\n",
      "\n",
      "Training FFNN with TF-IDF and 2 layers\n",
      "Epoch [1/10], Loss: 2126.0801639556885\n",
      "Epoch [2/10], Loss: 1998.6933007240295\n",
      "Epoch [3/10], Loss: 1963.53854906559\n",
      "Epoch [4/10], Loss: 1933.5398559570312\n",
      "Epoch [5/10], Loss: 1902.8719158172607\n",
      "Epoch [6/10], Loss: 1870.8874015808105\n",
      "Epoch [7/10], Loss: 1841.279080927372\n",
      "Epoch [8/10], Loss: 1816.0729239583015\n",
      "Epoch [9/10], Loss: 1797.565864264965\n",
      "Epoch [10/10], Loss: 1785.2200683951378\n",
      "Evaluation on Test Set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SaharKadkhodaMasoumA\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.32985, Precision: 0.23981533609427083, Recall: 0.3295950278342651, F1-Score: 0.2573339728174858\n",
      "\n",
      "Training FFNN with TF-IDF and 3 layers\n",
      "Epoch [1/10], Loss: 1472.6598109602928\n",
      "Epoch [2/10], Loss: 1261.7234600782394\n",
      "Epoch [3/10], Loss: 1173.2666721642017\n",
      "Epoch [4/10], Loss: 1074.9902056455612\n",
      "Epoch [5/10], Loss: 982.0924243330956\n",
      "Epoch [6/10], Loss: 927.5030905008316\n",
      "Epoch [7/10], Loss: 910.7739372253418\n",
      "Epoch [8/10], Loss: 910.7030127942562\n",
      "Epoch [9/10], Loss: 911.3091934025288\n",
      "Epoch [10/10], Loss: 909.6576625108719\n",
      "Evaluation on Test Set:\n",
      "Accuracy: 0.56975, Precision: 0.508187061069251, Recall: 0.5690386647150071, F1-Score: 0.5193990644210014\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SaharKadkhodaMasoumA\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "input_size_tfidf = X_train_tfidf.shape[1]\n",
    "output_size_tfidf = len(label_encoder)\n",
    "for num_layers in [1, 2, 3]:\n",
    "    print(f\"Training FFNN with TF-IDF and {num_layers} layers\")\n",
    "    ffnn_tfidf = FFNN(input_size_tfidf, 256, output_size_tfidf, num_layers)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(ffnn_tfidf.parameters(), lr=0.001)\n",
    "    train_model(ffnn_tfidf, criterion, optimizer, train_loader_tfidf, num_epochs=10)\n",
    "    print(\"Evaluation on Test Set:\")\n",
    "    evaluate_model(ffnn_tfidf, test_loader_tfidf)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FFNN with tfidf and LeakyReLU as an activation function\n",
      "Epoch [1/10], Loss: 1989.0983240008354\n",
      "Epoch [2/10], Loss: 1353.0579169988632\n",
      "Epoch [3/10], Loss: 1045.6689096987247\n",
      "Epoch [4/10], Loss: 918.1834497153759\n",
      "Epoch [5/10], Loss: 860.5946475565434\n",
      "Epoch [6/10], Loss: 805.3669632673264\n",
      "Epoch [7/10], Loss: 753.17565099895\n",
      "Epoch [8/10], Loss: 705.7292978316545\n",
      "Epoch [9/10], Loss: 669.6869882196188\n",
      "Epoch [10/10], Loss: 643.85013666749\n",
      "Evaluation on Test Set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SaharKadkhodaMasoumA\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.67525, Precision: 0.5958855356498138, Recall: 0.6769820138908375, F1-Score: 0.622319832409534\n",
      "\n",
      "Training FFNN with tfidf and ELU as an activation function\n",
      "Epoch [1/10], Loss: 1741.1844599843025\n",
      "Epoch [2/10], Loss: 1542.819808781147\n",
      "Epoch [3/10], Loss: 1493.7148686647415\n",
      "Epoch [4/10], Loss: 1455.0220013856888\n",
      "Epoch [5/10], Loss: 1417.0658017992973\n",
      "Epoch [6/10], Loss: 1376.7458293437958\n",
      "Epoch [7/10], Loss: 1336.772608935833\n",
      "Epoch [8/10], Loss: 1299.9106670618057\n",
      "Epoch [9/10], Loss: 1269.2381512522697\n",
      "Epoch [10/10], Loss: 1246.8699571490288\n",
      "Evaluation on Test Set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SaharKadkhodaMasoumA\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5538, Precision: 0.4752740156301395, Recall: 0.5567484209814866, F1-Score: 0.5018888406457622\n",
      "\n",
      "Training FFNN with tfidf and Sigmoid as an activation function\n",
      "Epoch [1/10], Loss: 1697.6441856622696\n",
      "Epoch [2/10], Loss: 1502.6167083978653\n",
      "Epoch [3/10], Loss: 1452.2089486718178\n",
      "Epoch [4/10], Loss: 1411.4019531607628\n",
      "Epoch [5/10], Loss: 1372.9568030238152\n",
      "Epoch [6/10], Loss: 1334.5735786557198\n",
      "Epoch [7/10], Loss: 1298.6759565472603\n",
      "Epoch [8/10], Loss: 1267.5561074614525\n",
      "Epoch [9/10], Loss: 1245.2560103535652\n",
      "Epoch [10/10], Loss: 1229.4111191034317\n",
      "Evaluation on Test Set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SaharKadkhodaMasoumA\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.58055, Precision: 0.48890891388077584, Recall: 0.5816266085872539, F1-Score: 0.5176493215945227\n",
      "\n",
      "Training FFNN with tfidf and Softmax as an activation function\n",
      "Epoch [1/10], Loss: 1077.9348317086697\n",
      "Epoch [2/10], Loss: 765.788494348526\n",
      "Epoch [3/10], Loss: 690.202105641365\n",
      "Epoch [4/10], Loss: 629.3187815696001\n",
      "Epoch [5/10], Loss: 568.375945404172\n",
      "Epoch [6/10], Loss: 508.4011527746916\n",
      "Epoch [7/10], Loss: 450.0943462923169\n",
      "Epoch [8/10], Loss: 400.73719269037247\n",
      "Epoch [9/10], Loss: 363.0493036210537\n",
      "Epoch [10/10], Loss: 337.2618256136775\n",
      "Evaluation on Test Set:\n",
      "Accuracy: 0.7244, Precision: 0.6799357441171696, Recall: 0.724543853929455, F1-Score: 0.696241392087842\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SaharKadkhodaMasoumA\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "input_size_tfidf = X_train_tfidf.shape[1]\n",
    "output_size_tfidf = len(label_encoder)\n",
    "activation_functions = [nn.LeakyReLU(), nn.ELU(), nn.Sigmoid(), nn.Softmax(dim=1)]\n",
    "\n",
    "for activation_function in activation_functions:\n",
    "    print(f\"Training FFNN with tfidf and {activation_function.__class__.__name__} as an activation function\")\n",
    "    ffnn_tfidf = FFNN(input_size_tfidf, 256, output_size_tfidf, num_layers=2)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(ffnn_tfidf.parameters(), lr=0.001)\n",
    "    train_model(ffnn_tfidf, criterion, optimizer, train_loader_tfidf, num_epochs=10)\n",
    "    print(\"Evaluation on Test Set:\")\n",
    "    evaluate_model(ffnn_tfidf, test_loader_tfidf)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model for evaluating hidden size\n",
    "import torch.nn as nn\n",
    "\n",
    "class FFNN_hidden_size(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, activation=nn.ReLU()):\n",
    "        super(FFNN_hidden_size, self).__init__()  # Corrected super call\n",
    "        self.num_layers = len(hidden_sizes) + 1\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(self.num_layers - 1):\n",
    "            if i == 0:\n",
    "                self.hidden_layers.append(nn.Linear(input_size, hidden_sizes[i]))\n",
    "            else:\n",
    "                self.hidden_layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))\n",
    "            self.hidden_layers.append(activation)\n",
    "        self.hidden_layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FFNN with CountVectorizer and 64 as hidden size\n",
      "Epoch [1/10], Loss: 941.8634288012981\n",
      "Epoch [2/10], Loss: 520.6390979588032\n",
      "Epoch [3/10], Loss: 429.6217685043812\n",
      "Epoch [4/10], Loss: 363.66021063923836\n",
      "Epoch [5/10], Loss: 308.9180175587535\n",
      "Epoch [6/10], Loss: 258.53121416270733\n",
      "Epoch [7/10], Loss: 214.06433810666203\n",
      "Epoch [8/10], Loss: 172.53173238411546\n",
      "Epoch [9/10], Loss: 135.47612247988582\n",
      "Epoch [10/10], Loss: 102.8923531267792\n",
      "Evaluation on Test Set:\n",
      "Accuracy: 0.8097, Precision: 0.8089457577534478, Recall: 0.8093464471654487, F1-Score: 0.8089794936234525\n",
      "\n",
      "Training FFNN with CountVectorizer and 128 as hidden size\n",
      "Epoch [1/10], Loss: 851.1252954006195\n",
      "Epoch [2/10], Loss: 507.1571571826935\n",
      "Epoch [3/10], Loss: 415.8249719515443\n",
      "Epoch [4/10], Loss: 347.1114614382386\n",
      "Epoch [5/10], Loss: 286.2970133051276\n",
      "Epoch [6/10], Loss: 228.85414877906442\n",
      "Epoch [7/10], Loss: 175.76912094652653\n",
      "Epoch [8/10], Loss: 128.94227942824364\n",
      "Epoch [9/10], Loss: 88.70155765675008\n",
      "Epoch [10/10], Loss: 58.024002687074244\n",
      "Evaluation on Test Set:\n",
      "Accuracy: 0.8069, Precision: 0.8059910760170789, Recall: 0.806425346835488, F1-Score: 0.8060951230097521\n",
      "\n",
      "Training FFNN with CountVectorizer and 256 as hidden size\n",
      "Epoch [1/10], Loss: 803.2735282480717\n",
      "Epoch [2/10], Loss: 497.434493213892\n",
      "Epoch [3/10], Loss: 404.6445658802986\n",
      "Epoch [4/10], Loss: 329.5851342007518\n",
      "Epoch [5/10], Loss: 259.1422016955912\n",
      "Epoch [6/10], Loss: 189.87424988672137\n",
      "Epoch [7/10], Loss: 130.8585676383227\n",
      "Epoch [8/10], Loss: 80.77442629914731\n",
      "Epoch [9/10], Loss: 46.501208796631545\n",
      "Epoch [10/10], Loss: 25.55829249927774\n",
      "Evaluation on Test Set:\n",
      "Accuracy: 0.80535, Precision: 0.8048004620261133, Recall: 0.8049636008402642, F1-Score: 0.8046219320176068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for count verctorize\n",
    "import torch.optim as optim\n",
    "\n",
    "input_size_count = X_train_count.shape[1]\n",
    "output_size_count = len(label_encoder)\n",
    "hidden_sizes = [64, 128, 256]\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    print(f\"Training FFNN with CountVectorizer and {hidden_size} as hidden size\")\n",
    "    ffnn_count = FFNN_hidden_size(input_size_count, [hidden_size], output_size_count, activation=nn.Sigmoid())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(ffnn_count.parameters(), lr=0.001)\n",
    "    train_model(ffnn_count, criterion, optimizer, train_loader_count, num_epochs=10)\n",
    "    print(\"Evaluation on Test Set:\")\n",
    "    evaluate_model(ffnn_count, test_loader_count)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "batch_sizes = [32, 64, 128, 256] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with batch size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SaharKadkhodaMasoumA\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4551, Precision: 0.3429175033707052, Recall: 0.4532065418237229, F1-Score: 0.36883909788361\n",
      "Accuracy with batch size 256: None\n",
      "Evaluating with batch size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SaharKadkhodaMasoumA\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.54675, Precision: 0.46408766866142354, Recall: 0.5481515266229686, F1-Score: 0.49083162620246945\n",
      "Accuracy with batch size 256: None\n",
      "Evaluating with batch size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SaharKadkhodaMasoumA\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.73005, Precision: 0.6856766014736746, Recall: 0.7301302971499337, F1-Score: 0.7016503641024141\n",
      "Accuracy with batch size 256: None\n",
      "Evaluating with batch size: 256\n",
      "Accuracy: 0.72445, Precision: 0.6800174701499333, Recall: 0.7235919316364209, F1-Score: 0.6952761456435674\n",
      "Accuracy with batch size 256: None\n",
      "Evaluation results:\n",
      "Batch size: 256, Accuracy: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SaharKadkhodaMasoumA\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "input_size_count = X_train_count.shape[1]\n",
    "output_size_count = len(label_encoder)\n",
    "# Iterate over different batch sizes\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"Evaluating with batch size: {batch_size}\")\n",
    "    \n",
    "    # Define the model\n",
    "    model = FFNN(input_size_count, 256, output_size_count, num_layers=2, activation=nn.ReLU())\n",
    "    \n",
    "    # Define the loss criterion and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for batch_size in batch_sizes: \n",
    "        train_loader_count = DataLoader(train_dataset_count, batch_size, shuffle=True)\n",
    "        test_loader_count = DataLoader(test_dataset_count, batch_size, shuffle=False)\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader_count:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.float()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = evaluate_model(model, test_loader_count)\n",
    "    results[batch_size] = accuracy\n",
    "    print(f\"Accuracy with batch size {batch_size}: {accuracy}\")\n",
    "\n",
    "# Print the results\n",
    "print(\"Evaluation results:\")\n",
    "for batch_size, accuracy in results.items():\n",
    "    print(f\"Batch size: {batch_size}, Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
